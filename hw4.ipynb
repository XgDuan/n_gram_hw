{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算语言学第四章作业\n",
    "\n",
    "## 段续光 2018310786\n",
    "\n",
    "### 分析\n",
    "具体的细节和分析在下面。这里给出结论，首先，我们发现good-turing和adding-oneg相比，在unigram上差别不大，但是由于bigram中我们需要考虑到大量的两个相连但是其共同出现概率很小的情况，good-turing在低频词语处理较好，因此达到了很好的效果。然后在按照汉字分词后，效果明显变差，这是因为以汉字分词后bigram其实是词语（也有不是词语的），失去了本来的意义，就像我们对英文使用26个字母分词一样，最终必须使用很大的ngram才可能达到不错的效果。\n",
    "\n",
    "\n",
    "### adding-one smoothing\n",
    "对于不存在的token(bin)进行加一处理是非常直观简单的策略。其结果如下：\n",
    "```\n",
    "unigram: val:  2738.5636864614294\n",
    "unigram test:  2578.6525391739356\n",
    "bigram val:    61.30511939104798\n",
    "bigram test:   60.83866751076157\n",
    "```\n",
    "#### 其他测试\n",
    "此外，我们给出了困惑度最高和最低的5个句子，如下：\n",
    "##### unigram\n",
    "```\n",
    "sent with minimum perplexity:\n",
    "１/m  \n",
    "３/m  \n",
    "图片/n  ：/w  \n",
    "４/m  \n",
    "２/m  \n",
    "sent with maximum perplexity:\n",
    "每逢/v  佳节/n  倍/d  思亲/v  重洋/n  阻隔/v  心/n  相连/v  宋/nr  健/nr  电话/n  慰问/v  南极/ns  考察队员/n  \n",
    "三塚/nr  被迫/d  提交/v  辞呈/n  桥本/nr  兼任/v  大藏/n  大臣/n  \n",
    "西藏/ns  边防/b  部队/n  风/n  歌/Vg  雪/n  舞/v  庆/Vg  佳节/n  [总后/j  青藏/j  兵站部/n]nt  抢运/v  藏北/s  救灾/vn  物资/n  \n",
    "星/n  拱/v  北辰/n  励精图治/i  寅/Mg  虎啸/n  来/v  生气/n  珠/j  还/d  南/f  浦/j  除旧迎新/l  丑牛/t  歇/v  过/u  忙/a  时/Ng  （/w  行书/n  ）/w  \n",
    "英国/ns  可怜/vn  鱼儿/n  雄/Bg  变/v  雌/b  奈何/v  污水/n  肆意/d  流/v  \n",
    "```\n",
    "##### bigram\n",
    "```\n",
    "sent with minimum perplexity:\n",
    "（/w  新华社/nt  记者/n  兰/nr  红光/nr  摄/Vg  ）/w  \n",
    "虎年/t  虎园/n  听/v  虎啸/n  （/w  附/v  图片/n  １/m  张/q  ）/w  \n",
    "（/w  新华社/nt  记者/n  陈/nr  建力/nr  摄/Vg  ）/w  \n",
    "礼花/n  映/v  红/a  京郊/j  夜/Tg  （/w  附/v  图片/n  １/m  张/q  ）/w  \n",
    "牟/nr  爱牧/nr  同志/n  逝世/v  （/w  附/v  图片/n  １/m  张/q  ）/w  \n",
    "sent with maximum perplexity:\n",
    "是/v  清冽/a  的/u  夜/Tg  ，/w  \n",
    "成为/v  生动/a  的/u  立体/b  ，/w  \n",
    "和/c  嘴唇/n  \n",
    "代表/n  代表/n  \n",
    "和/c  明媚/a  的/u  江南/ns\n",
    "```\n",
    "我们发现unigram情况下，给出的最好的句子（perplexity）都是高频词，而bigram下则给出的是更加真实的句子。但是因为分句方式不合适，出现了“代表 代表”这样的句子。\n",
    "\n",
    "### good-turing smoothing\n",
    "good-turing smoothing相对于add-one最大的优化是针对出现次数较少的词的，这些词事实上并非真的少见，good-turning smoothing事实上调高了他们出现的概率。其结果如下：\n",
    "```\n",
    "unigram: val:  1563.9899201180328\n",
    "unigram test:  1529.6967566474705\n",
    "bigram val:    0.783308880539215\n",
    "bigram test:   0.909185314042866\n",
    "```\n",
    "相比于adding-one，unigram的差别不大，但是bigram的差别变得特别大，这其实也说明了n-gram优于unigram。\n",
    "其他测试可以通过脚本得到。我们在这里张贴结果：\n",
    "##### 最大最小概率句子：\n",
    "##### unigram\n",
    "```\n",
    "sent with minimum perplexity:\n",
    "西坑村/ns  的/u  领路人/n  \n",
    "鄂/nr  青子/nr  \n",
    "竹青/nr  \n",
    "稀释/v  后/f  ，/w  \n",
    "（/w  大康/nr  ）/w  \n",
    "sent with maximum perplexity:\n",
    "三峡/ns  工地/n  战/Ng  犹/d  酣/Ag  \n",
    "美/j  高官/n  四处/d  游说/v  俄/j  与/c  法/j  反对/v  动武/v  伊拉克/ns  欲/d  诉/Vg  公堂/n  \n",
    "每逢/v  佳节/n  倍/d  思亲/v  重洋/n  阻隔/v  心/n  相连/v  宋/nr  健/nr  电话/n  慰问/v  南极/ns  考察队员/n  \n",
    "勿/d  滥用/v  胡萝卜素/n  药片/n  \n",
    "火树银花/i  迎/v  新岁/t  同心协力/i  建/v  香江/ns  香港/ns  举行/v  盛大/b  贺岁/vn  烟花/n  汇演/vn \n",
    "```\n",
    "##### bigram\n",
    "```\n",
    "\n",
    "sent with minimum perplexity:\n",
    "三峡/ns  工地/n  战/Ng  犹/d  酣/Ag  \n",
    "涉嫌/v  接受/v  金融/n  机构/n  贿赂/vn  [日/j  大藏省/nt]nt  两/m  官员/n  被捕/v  \n",
    "公仆/n  深情/n  洒/v  云/n  山/n  \n",
    "忽/d  接/v  乡/n  书/n  喜/v  不/d  禁/v  \n",
    "除夕/t  守/v  岗/n  送/v  光明/n  \n",
    "sent with maximum perplexity:\n",
    "１/m \n",
    "３/m \n",
    "难忘/a  的/u  歌/n \n",
    "你/r  就/d  说/v \n",
    "４/m \n",
    "```\n",
    "\n",
    "### 使用字符级别重新计算\n",
    "\n",
    "\n",
    "### adding-one smoothing\n",
    "对于不存在的token(bin)进行加一处理是非常直观简单的策略。其结果如下：\n",
    "```\n",
    "unigram: val:  713.1504563162416\n",
    "unigram test:  737.1501166530528\n",
    "bigram val:    80.7637245977163\n",
    "bigram test:   79.37076759636406\n",
    "```\n",
    "#### 其他测试\n",
    "此外，我们给出了困惑度最高和最低的5个句子，如下：\n",
    "##### unigram\n",
    "```\n",
    "sent with minimum perplexity:\n",
    "４/m  \n",
    "面人/n  \n",
    "（/w  Ａ/nx  、/w  Ｂ/nx  ）/w  \n",
    "成为/v  生动/a  的/u  立体/b  ，/w  \n",
    "１９９７年/t  １２月/t  ２９日/t  \n",
    "sent with maximum perplexity:\n",
    "她/r  宁肯/d  忍受/v  不/d  贞洁/n  的/u  咒骂/vn  \n",
    "丑牛/t  奋/Vg  蹄/Ng  开/v  锦绣/b  寅虎/n  添/v  翼/Ng  会/v  风云/n  （/w  隶书/n  ）/w  \n",
    "勿/d  滥用/v  胡萝卜素/n  药片/n  \n",
    "温/nr  妮妮/nr  \n",
    "悄悄/d  吻/v  湿/a  \n",
    "\n",
    "```\n",
    "##### bigram\n",
    "```\n",
    "sent with minimum perplexity:\n",
    "（/w  新华社/nt  北京/ns  １月/t  ２９日/t  电/n  ）/w  \n",
    "（/w  新华社/nt  北京/ns  １９９７年/t  １２月/t  ３０日/t  电/n  ）/w  \n",
    "（/w  本报/r  记者/n  王/nr  霞光/nr  摄/Vg  ）/w  \n",
    "（/w  新华社/nt  记者/n  周/nr  浩/nr  摄/Vg  ）/w  \n",
    "（/w  新华社/nt  记者/n  戴/nr  浩/nr  摄/Vg  ）/w  \n",
    "sent with maximum perplexity:\n",
    "赶/v  戏/n  \n",
    "忽/d  接/v  乡/n  书/n  喜/v  不/d  禁/v  \n",
    "等/v  着/u  你/r  \n",
    "观雪/v  \n",
    "挥师/v  金马河/ns  \n",
    "\n",
    "```\n",
    "我们发现unigram情况下，给出的最好的句子（perplexity）都是高频词，而bigram下则给出的是更加真实的句子。但是因为分句方式不合适，出现了“代表 代表”这样的句子。\n",
    "\n",
    "### good-turing smoothing\n",
    "good-turing smoothing相对于add-one最大的优化是针对出现次数较少的词的，这些词事实上并非真的少见，good-turning smoothing事实上调高了他们出现的概率。其结果如下：\n",
    "```\n",
    "unigram: val:  710.6985830694932\n",
    "unigram test:  733.7886269800006\n",
    "bigram val:    20.21308428540911\n",
    "bigram test:   19.245991733556036\n",
    "```\n",
    "相比于adding-one，unigram的差别不大，但是bigram的差别变得特别大，这其实也说明了n-gram优于unigram。\n",
    "其他测试可以通过脚本得到。我们在这里张贴结果：\n",
    "##### 最大最小概率句子：\n",
    "##### unigram\n",
    "```\n",
    "sent with minimum perplexity:\n",
    "４/m  \n",
    "面人/n  \n",
    "（/w  Ａ/nx  、/w  Ｂ/nx  ）/w  \n",
    "成为/v  生动/a  的/u  立体/b  ，/w  \n",
    "１９９７年/t  １２月/t  ２９日/t  \n",
    "sent with maximum perplexity:\n",
    "戊寅/t  吉祥/n  （/w  篆刻/n  ）/w  \n",
    "丑牛/t  奋/Vg  蹄/Ng  开/v  锦绣/b  寅虎/n  添/v  翼/Ng  会/v  风云/n  （/w  隶书/n  ）/w  \n",
    "勿/d  滥用/v  胡萝卜素/n  药片/n  \n",
    "悄悄/d  吻/v  湿/a  \n",
    "温/nr  妮妮/nr  \n",
    "```\n",
    "##### bigram\n",
    "```\n",
    "sent with minimum perplexity:\n",
    "悠棠/nr  \n",
    "温/nr  妮妮/nr  \n",
    "新疆/ns  戍边/vn  官兵/n  哨卡/n  守岁/v  \n",
    "（/w  佟/nr  韦/nr  ）/w  \n",
    "李/nr  英桃/nr  \n",
    "sent with maximum perplexity:\n",
    "和/c  明媚/a  的/u  江南/ns  \n",
    "面人/n  \n",
    "成/nr  志伟/nr  \n",
    "宋/nr  志坚/nr  \n",
    "季/nr  音/nr \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix source code\n",
    "\n",
    "## Note:\n",
    "+ 因为某些原因，所以代码中存在很多重名函数，所以如果结果错了，从头运行即可\n",
    "+ 上述作业中出现了的例子在下面的代码中都有对应的测试点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "import re\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = 'datas/train/'\n",
    "valid_dir = 'datas/valid/'\n",
    "test_dir = 'datas/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', '年', '根', '儿', '了', '，', '一', '张', '张', '贺', '卡', '，', '飞', '越', '关', '山', '。', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "bos = '<BOS>'\n",
    "eos  ='<EOS>'\n",
    "\n",
    "def word_split(line):\n",
    "    \"\"\"\n",
    "    line: a line of words\n",
    "    \"\"\"\n",
    "    # return line.split()  \n",
    "    return [bos] + line.split() + [eos]  # append bos, eos\n",
    "\n",
    "# 使用下面的word split函数重新运行所有代码即得到第四题的结果\n",
    "# def word_split(line):\n",
    "#     \"\"\"\n",
    "#     line: a line of words\n",
    "#     \"\"\"\n",
    "#     # return line.split()\n",
    "#     return [bos] + re.findall(r'[\\u4e00-\\u9fa5，。、；！：（）《》“”？—_]', line) + [eos]  # append bos, eos\n",
    "\n",
    "\n",
    "def bigram_construct(words):\n",
    "    \"\"\"\n",
    "    words: [w1, w2, w3..]\n",
    "    \"\"\"\n",
    "    return [words[0]] + [' '.join(x) for x in zip(words[:-1], words[1:])]\n",
    "\n",
    "def geo_mean(values):\n",
    "    return math.exp(1.0 / len(values) * sum([math.log(p) for p in values])) \n",
    "\n",
    "print(word_split('年根儿/n  了/y  ，/w  一张张/m  贺卡/n  ，/w  飞越/v  关山/n 。/m '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. adding-one smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_freq_add1(dataset_path):\n",
    "    word_list = []\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        with open(dataset_path + dir_name, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                word_list.append(word_split(line))\n",
    "    word_counter = Counter(chain(*word_list))\n",
    "    total_word = sum(word_counter.values())  # N\n",
    "    word_freq = defaultdict(lambda: 1.0 / (len(word_counter) + total_word))\n",
    "    for word in word_counter:\n",
    "        word_freq[word] = (word_counter[word] + 1.0) / (total_word + len(word_counter))\n",
    "    return word_counter, word_freq\n",
    "\n",
    "def unigram_test_unused(dataset_path, freq_model):\n",
    "    # the error of the two methods is less than 10^{-17} in the total dataset\n",
    "    perplexity_dict = dict()\n",
    "    def perplexity(sentence):\n",
    "        words = word_split(sentence)\n",
    "        len_word = len(words)\n",
    "        return reduce(lambda x, y: x*y, [math.pow(freq_model[word], -1.0/len_word) for word in words])\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        with open(dataset_path + dir_name, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                perplexity_dict[line] = perplexity(line)\n",
    "    return perplexity_dict\n",
    "\n",
    "def unigram_test(dataset_path, freq_model):\n",
    "    perplexity_dict = dict()\n",
    "    def perplexity(sentence):\n",
    "        words = word_split(sentence)\n",
    "        len_word = len(words)\n",
    "        return math.exp(-1.0/len_word * sum([math.log(freq_model[word]) for word in words]))\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        with open(dataset_path + dir_name, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                perplexity_dict[line] = perplexity(line)\n",
    "    return perplexity_dict\n",
    "\n",
    "def unigram_test_sent(sentence, freq_model):\n",
    "    def perplexity(sentence):\n",
    "        words = word_split(sentence)\n",
    "        bigrams = bigram_construct(words)\n",
    "        len_word = len(words)\n",
    "        return math.exp(-1.0/len_word * (sum([-math.log(freq_model[word]) for word in words[:-1]] + \n",
    "                                             [math.log(freq_model[bigram]) for bigram in bigrams])))\n",
    "    return perplexity(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIGRAM\n",
    "###### 这里的N和B按照网络学堂（而不是课件4-1P14）要求给出，即`N = num(total_ngram), B = num(set(total_ngram))`，二者相差常数倍\n",
    "$$\n",
    "Perplexity(w0,w2,...,w_N) =(p(w_0) \\prod_{i=0}^{N-1}p(w_{i+1}|w_i))^{-1/N} = exp(-\\frac{1}{N}(\\sum \\log{p(w_i, w_{i+1})} -\\sum \\log{p(w_i)} + \\log{p(w_0)} - \\log{p_{w_N}}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_freq_add1(dataset_path):\n",
    "    word_list = []\n",
    "    # word_count = []\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        with open(dataset_path + dir_name, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                _word_split = word_split(line)\n",
    "                # word_count.append(_word_split)\n",
    "                word_list.append(_word_split)\n",
    "                word_list.append(bigram_construct(_word_split)[1:])  # remove the <BOS> in the begining\n",
    "    word_counter = Counter(chain(*word_list))\n",
    "    total_word = sum(word_counter.values())  # N\n",
    "    word_freq = defaultdict(lambda: 1.0 / (len(word_counter) + total_word))  # 1/ (N + B) \n",
    "    for word in word_counter:\n",
    "        word_freq[word] = (word_counter[word] + 1.0) / (len(word_counter) + total_word)\n",
    "    return word_counter, word_freq\n",
    "\n",
    "def bigram_test(dataset_path, freq_model):\n",
    "    perplexity_dict = dict()\n",
    "    def perplexity(sentence):\n",
    "        words = word_split(sentence)\n",
    "        bigrams = bigram_construct(words)\n",
    "        len_word = len(words)\n",
    "        return math.exp(-1.0/len_word * (sum([-math.log(freq_model[word]) for word in words[:-1]] + \n",
    "                                             [math.log(freq_model[bigram]) for bigram in bigrams])))\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        with open(dataset_path + dir_name, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                perplexity_dict[line] = perplexity(line)\n",
    "    return perplexity_dict\n",
    "\n",
    "def bigram_test_sent(sentence, freq_model):\n",
    "    def perplexity(sentence):\n",
    "        words = word_split(sentence)\n",
    "        bigrams = bigram_construct(words)\n",
    "        len_word = len(words)\n",
    "        return math.exp(-1.0/len_word * (sum([-math.log(freq_model[word]) for word in words[:-1]] + \n",
    "                                             [math.log(freq_model[bigram]) for bigram in bigrams])))\n",
    "    return perplexity(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram: val:  713.1504563162416\n",
      "unigram test:  737.1501166530528\n",
      "bigram val:    80.7637245977163\n",
      "bigram test:   79.37076759636406\n"
     ]
    }
   ],
   "source": [
    "w_c_u, w_f_u = unigram_freq_add1(train_dir)\n",
    "w_c_b, w_f_b = bigram_freq_add1(train_dir)\n",
    "unigram_valid_perplexities = unigram_test(valid_dir, w_f_u)\n",
    "unigram_test_perplexities = unigram_test(test_dir, w_f_u)\n",
    "bigram_valid_perplexities = bigram_test(valid_dir, w_f_b)\n",
    "bigram_test_perplexities = bigram_test(test_dir, w_f_b)\n",
    "\n",
    "unigram_valid_perplexity = geo_mean(unigram_valid_perplexities.values())\n",
    "unigram_test_perplexity = geo_mean(unigram_test_perplexities.values())\n",
    "bigram_valid_perplexity = geo_mean(bigram_valid_perplexities.values())\n",
    "bigram_test_perplexity = geo_mean(bigram_test_perplexities.values())\n",
    "\n",
    "print(\"unigram: val: \", unigram_valid_perplexity)\n",
    "print(\"unigram test: \", unigram_test_perplexity)\n",
    "print(\"bigram val:   \", bigram_valid_perplexity)\n",
    "print(\"bigram test:  \", bigram_test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sentence with minimum/maximum perplexity(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent with minimum perplexity:\n",
      "４/m  \n",
      "\n",
      "面人/n  \n",
      "\n",
      "（/w  Ａ/nx  、/w  Ｂ/nx  ）/w  \n",
      "\n",
      "成为/v  生动/a  的/u  立体/b  ，/w  \n",
      "\n",
      "１９９７年/t  １２月/t  ２９日/t  \n",
      "\n",
      "sent with maximum perplexity:\n",
      "她/r  宁肯/d  忍受/v  不/d  贞洁/n  的/u  咒骂/vn  \n",
      "\n",
      "丑牛/t  奋/Vg  蹄/Ng  开/v  锦绣/b  寅虎/n  添/v  翼/Ng  会/v  风云/n  （/w  隶书/n  ）/w  \n",
      "\n",
      "勿/d  滥用/v  胡萝卜素/n  药片/n  \n",
      "\n",
      "温/nr  妮妮/nr  \n",
      "\n",
      "悄悄/d  吻/v  湿/a  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "perplexity_sent = {val:key for key, val in unigram_test_perplexities.items()}\n",
    "perplexity_sort = sorted(perplexity_sent.keys()) \n",
    "head_5 = perplexity_sort[:5]\n",
    "tail_5 = perplexity_sort[-5:]\n",
    "\n",
    "print(\"sent with minimum perplexity:\")\n",
    "for key in head_5:\n",
    "    print(perplexity_sent[key])\n",
    "    \n",
    "    \n",
    "print(\"sent with maximum perplexity:\")\n",
    "for key in tail_5:\n",
    "    print(perplexity_sent[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sentence with maximum perplexity(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent with minimum perplexity:\n",
      "（/w  新华社/nt  北京/ns  １月/t  ２９日/t  电/n  ）/w  \n",
      "\n",
      "（/w  新华社/nt  北京/ns  １９９７年/t  １２月/t  ３０日/t  电/n  ）/w  \n",
      "\n",
      "（/w  本报/r  记者/n  王/nr  霞光/nr  摄/Vg  ）/w  \n",
      "\n",
      "（/w  新华社/nt  记者/n  周/nr  浩/nr  摄/Vg  ）/w  \n",
      "\n",
      "（/w  新华社/nt  记者/n  戴/nr  浩/nr  摄/Vg  ）/w  \n",
      "\n",
      "sent with maximum perplexity:\n",
      "赶/v  戏/n  \n",
      "\n",
      "忽/d  接/v  乡/n  书/n  喜/v  不/d  禁/v  \n",
      "\n",
      "等/v  着/u  你/r  \n",
      "\n",
      "观雪/v  \n",
      "\n",
      "挥师/v  金马河/ns  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "perplexity_sent = {val:key for key, val in bigram_test_perplexities.items()}\n",
    "perplexity_sort = sorted(perplexity_sent.keys()) \n",
    "head_5 = perplexity_sort[:5]\n",
    "tail_5 = perplexity_sort[-5:]\n",
    "\n",
    "print(\"sent with minimum perplexity:\")\n",
    "for key in head_5:\n",
    "    print(perplexity_sent[key])\n",
    "    \n",
    "    \n",
    "print(\"sent with maximum perplexity:\")\n",
    "for key in tail_5:\n",
    "    print(perplexity_sent[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. good-turing smoothing\n",
    "$$\n",
    "P(w1,w2,w3..) = \\left\\{ \\begin{array}{ll}\n",
    "\\frac{r^*}{N},~where~N=\\#\\{n\\_gram\\},~r^*=(C(w1,w2,w3..)+1)\\frac{N_{r+1}}{N_r}, & r<N_{thres}\\\\\n",
    "\\frac{r}{N},~where~r=C(w1,w2,w3,..) & else\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SINGLE GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_thres = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def unigram_freq_gts(dataset_path):\n",
    "    word_list = []\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        with open(dataset_path + dir_name, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                word_list.append(word_split(line))\n",
    "    word_counter = Counter(chain(*word_list))\n",
    "    S_fn = Counter(word_counter.values())    # ignore the smoothing\n",
    "    total_word = sum(word_counter.values())  # N\n",
    "    word_freq = defaultdict(lambda: S_fn[1] / total_word)  # S_fn[0] = 1\n",
    "    for word in word_counter:\n",
    "        if word_counter[word] < N_thres:\n",
    "            word_freq[word] = (word_counter[word] + 1.0) * (S_fn[word_counter[word] + 1] * 1.0 / S_fn[word_counter[word]]) / total_word\n",
    "        else:\n",
    "            word_freq[word] = word_counter[word] *1.0 / total_word\n",
    "    return word_counter, word_freq\n",
    "\n",
    "def unigram_test(dataset_path, freq_model):\n",
    "    perplexity_dict = dict()\n",
    "    def perplexity(sentence):\n",
    "        words = word_split(sentence)\n",
    "        len_word = len(words)\n",
    "        return math.exp(-1.0/len_word * sum([math.log(freq_model[word]) for word in words]))\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        with open(dataset_path + dir_name, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                perplexity_dict[line] = perplexity(line)\n",
    "    return perplexity_dict\n",
    "\n",
    "def unigram_test_sent(sentence, freq_model):\n",
    "    def perplexity(sentence):\n",
    "        words = word_split(sentence)\n",
    "        bigrams = bigram_construct(words)\n",
    "        len_word = len(words)\n",
    "        return math.exp(-1.0/len_word * (sum([-math.log(freq_model[word]) for word in words[:-1]] + \n",
    "                                             [math.log(freq_model[bigram]) for bigram in bigrams])))\n",
    "    return perplexity(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_freq_gts(dataset_path):\n",
    "    word_list = []\n",
    "    # word_count = []\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        with open(dataset_path + dir_name, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                _word_split = word_split(line)\n",
    "                # word_count.append(_word_split)\n",
    "                word_list.append(_word_split)\n",
    "                word_list.append(bigram_construct(_word_split)[1:])  # remove the <BOS> in the begining\n",
    "    word_counter = Counter(chain(*word_list))\n",
    "    S_fn = Counter(word_counter.values())    # ignore the smoothing \n",
    "    total_word = sum(word_counter.values())  # N\n",
    "    word_freq = defaultdict(lambda: S_fn[1] / total_word)  # S_fn[0] = 1\n",
    "    for word in word_counter:\n",
    "        if word_counter[word] < N_thres:\n",
    "            word_freq[word] = (word_counter[word] + 1.0) * (S_fn[word_counter[word] + 1] * 1.0 / S_fn[word_counter[word]]) / total_word\n",
    "        else:\n",
    "            word_freq[word] = word_counter[word] *1.0 / total_word\n",
    "    return word_counter, word_freq\n",
    "\n",
    "def bigram_test(dataset_path, freq_model):\n",
    "    perplexity_dict = dict()\n",
    "    def perplexity(sentence):\n",
    "        words = word_split(sentence)\n",
    "        bigrams = bigram_construct(words)\n",
    "        len_word = len(words)\n",
    "        return math.exp(-1.0/len_word * (sum([-math.log(freq_model[word]) for word in words[:-1]] + \n",
    "                                             [math.log(freq_model[bigram]) for bigram in bigrams])))\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        with open(dataset_path + dir_name, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                perplexity_dict[line] = perplexity(line)\n",
    "    return perplexity_dict\n",
    "\n",
    "def bigram_test_sent(sentence, freq_model):\n",
    "    def perplexity(sentence):\n",
    "        words = word_split(sentence)\n",
    "        bigrams = bigram_construct(words)\n",
    "        len_word = len(words)\n",
    "        return math.exp(-1.0/len_word * (sum([-math.log(freq_model[word]) for word in words[:-1]] + \n",
    "                                             [math.log(freq_model[bigram]) for bigram in bigrams])))\n",
    "    return perplexity(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram: val:  710.6985830694932\n",
      "unigram test:  733.7886269800006\n",
      "bigram val:    20.21308428540911\n",
      "bigram test:   19.245991733556036\n"
     ]
    }
   ],
   "source": [
    "w_c_u, w_f_u = unigram_freq_gts(train_dir)\n",
    "w_c_b, w_f_b = bigram_freq_gts(train_dir)\n",
    "unigram_valid_perplexities = unigram_test(valid_dir, w_f_u)\n",
    "unigram_test_perplexities = unigram_test(test_dir, w_f_u)\n",
    "bigram_valid_perplexities = bigram_test(valid_dir, w_f_b)\n",
    "bigram_test_perplexities = bigram_test(test_dir, w_f_b)\n",
    "\n",
    "unigram_valid_perplexity = math.exp(1.0 / len(unigram_valid_perplexities) * sum([math.log(p) for p in unigram_valid_perplexities.values()]))\n",
    "unigram_test_perplexity = math.exp(1.0 / len(unigram_test_perplexities) * sum([math.log(p) for p in unigram_test_perplexities.values()]))\n",
    "bigram_valid_perplexity = math.exp(1.0 / len(bigram_valid_perplexities) * sum([math.log(p) for p in bigram_valid_perplexities.values()]))\n",
    "bigram_test_perplexity = math.exp(1.0 / len(bigram_test_perplexities) * sum([math.log(p) for p in bigram_test_perplexities.values()]))\n",
    "\n",
    "print(\"unigram: val: \", unigram_valid_perplexity)\n",
    "print(\"unigram test: \", unigram_test_perplexity)\n",
    "print(\"bigram val:   \", bigram_valid_perplexity)\n",
    "print(\"bigram test:  \", bigram_test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sentence with minimum/maximum perplexity(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent with minimum perplexity:\n",
      "４/m  \n",
      "\n",
      "面人/n  \n",
      "\n",
      "（/w  Ａ/nx  、/w  Ｂ/nx  ）/w  \n",
      "\n",
      "成为/v  生动/a  的/u  立体/b  ，/w  \n",
      "\n",
      "１９９７年/t  １２月/t  ２９日/t  \n",
      "\n",
      "sent with maximum perplexity:\n",
      "戊寅/t  吉祥/n  （/w  篆刻/n  ）/w  \n",
      "\n",
      "丑牛/t  奋/Vg  蹄/Ng  开/v  锦绣/b  寅虎/n  添/v  翼/Ng  会/v  风云/n  （/w  隶书/n  ）/w  \n",
      "\n",
      "勿/d  滥用/v  胡萝卜素/n  药片/n  \n",
      "\n",
      "悄悄/d  吻/v  湿/a  \n",
      "\n",
      "温/nr  妮妮/nr  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "perplexity_sent = {val:key for key, val in unigram_test_perplexities.items()}\n",
    "perplexity_sort = sorted(perplexity_sent.keys()) \n",
    "head_5 = perplexity_sort[:5]\n",
    "tail_5 = perplexity_sort[-5:]\n",
    "\n",
    "print(\"sent with minimum perplexity:\")\n",
    "for key in head_5:\n",
    "    print(perplexity_sent[key])\n",
    "    \n",
    "    \n",
    "print(\"sent with maximum perplexity:\")\n",
    "for key in tail_5:\n",
    "    print(perplexity_sent[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sentence with minimum/maximum perplexity(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent with minimum perplexity:\n",
      "悠棠/nr  \n",
      "\n",
      "温/nr  妮妮/nr  \n",
      "\n",
      "新疆/ns  戍边/vn  官兵/n  哨卡/n  守岁/v  \n",
      "\n",
      "（/w  佟/nr  韦/nr  ）/w  \n",
      "\n",
      "李/nr  英桃/nr  \n",
      "\n",
      "sent with maximum perplexity:\n",
      "和/c  明媚/a  的/u  江南/ns  \n",
      "\n",
      "面人/n  \n",
      "\n",
      "成/nr  志伟/nr  \n",
      "\n",
      "宋/nr  志坚/nr  \n",
      "\n",
      "季/nr  音/nr  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "perplexity_sent = {val:key for key, val in bigram_test_perplexities.items()}\n",
    "perplexity_sort = sorted(perplexity_sent.keys()) \n",
    "head_5 = perplexity_sort[:5]\n",
    "tail_5 = perplexity_sort[-5:]\n",
    "\n",
    "print(\"sent with minimum perplexity:\")\n",
    "for key in head_5:\n",
    "    print(perplexity_sent[key])\n",
    "    \n",
    "    \n",
    "print(\"sent with maximum perplexity:\")\n",
    "for key in tail_5:\n",
    "    print(perplexity_sent[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
